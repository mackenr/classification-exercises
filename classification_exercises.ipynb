{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from acquire import *\n",
    "from rcm_library import *\n",
    "\n",
    "# import lux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Exercises\n",
    "\n",
    "The end product of this exercise should be the specified functions in a python script named prepare.py. Do these in your classification_exercises.ipynb first, then transfer to the prepare.py file.\n",
    "\n",
    "This work should all be saved in your local classification-exercises repo. Then add, commit, and push your changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Iris Data:\n",
    "\n",
    ">1. Use the function defined in acquire.py to load the iris data. -->\n",
    "\n",
    ">2. Drop the species_id and measurement_id columns.\n",
    "\n",
    ">3. Rename the species_name column to just species.\n",
    "\n",
    ">4. Create dummy variables of the species name and concatenate onto the iris dataframe. (This is for practice, we don't always have to encode the target, but if we used species as a feature, we would need to encode it).\n",
    "\n",
    ">5. Create a function named prep_iris that accepts the untransformed iris data, and returns the data with the transformations above applied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_database_info_probe('iris_db')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# iris=get_iris_data()\n",
    "\n",
    "# pulled_db(iris)\n",
    "\n",
    "# uploaded_db_explore(iris)\n",
    "\n",
    "# numcols(iris)\n",
    "# iris=prep_iris(iris)\n",
    "# iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Titanic dataset\n",
    "\n",
    ">1. Use the function defined in acquire.py to load the Titanic data.\n",
    "\n",
    ">2. Drop any unnecessary, unhelpful, or duplicated columns.\n",
    "\n",
    ">3. Encode the categorical columns. Create dummy variables of the categorical columns and concatenate them onto the dataframe.\n",
    "\n",
    ">4. Create a function named prep_titanic that accepts the raw titanic data, and returns the data with the transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_database_info_probe('titanic_db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanic=get_titanic_data()\n",
    "# titanic=prep_titanic(titanic)\n",
    "# titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Telco dataset\n",
    "\n",
    ">1. Use the function defined in acquire.py to load the Telco data.\n",
    "\n",
    ">2. Drop any unnecessary, unhelpful, or duplicated columns. This could mean dropping foreign key columns but keeping the corresponding string values, for example.\n",
    "\n",
    ">3. Encode the categorical columns. Create dummy variables of the categorical columns and concatenate them onto the dataframe.\n",
    "\n",
    ">4. Create a function named prep_telco that accepts the raw telco data, and returns the data with the transformations above applied.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_database_info_probe('telco_churn')\n",
    "\n",
    "telco=get_telco_data()\n",
    "# uploaded_db_explore(telco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numcols(telco)\n",
    "\n",
    "##use crosstab to see if there is one to one realationship\n",
    "## create a function using a loop and  combinations to see all the cross tab relationships\n",
    "\n",
    "# list,values=crosstabs_todrop(telco)\n",
    "\n",
    "\n",
    "\n",
    "telco=prep_telco(telco)\n",
    "telco.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split your data\n",
    "\n",
    ">1. Write a function to split your data into train, test and validate datasets. Add this function to prepare.py.\n",
    "\n",
    ">2. Run the function in your notebook on the Iris dataset, returning 3 datasets, train_iris, validate_iris and test_iris.\n",
    "\n",
    ">3. Run the function on the Titanic dataset, returning 3 datasets, train_titanic, validate_titanic and test_titanic.\n",
    "\n",
    ">4. Run the function on the Telco dataset, returning 3 datasets, train_telco, validate_telco and test_telco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iris,validate_iris,test_iris=split_data(iris)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_titanic,validate_titanic,test_titanic =split_data(titanic,tostratify='survived_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_telco,validate_telco,test_telco =split_data(telco)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis Exercises \n",
    "\n",
    "## Part I\n",
    "\n",
    ">Continue in your classification_exercises.ipynb notebook. As always, add, commit, and push your changes.\n",
    "\n",
    ">Section 1 - iris_db: Using iris data from our mySQL server and the methods used in the lesson above:\n",
    "\n",
    ">Acquire, prepare & split your data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Univariate Stats\n",
    "\n",
    ">For each measurement type (quantitative variable): create a histogram, boxplot, & compute descriptive statistics (using .describe()).\n",
    "\n",
    ">For each species (categorical variable): create a frequency table and a bar plot of those frequencies.\n",
    "\n",
    ">Document takeaways & any actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iris.head()\n",
    "# # univariate(train_iris)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Bivariate Stats\n",
    "\n",
    ">Visualize each measurement type (y-axis) with the species variable (x-axis) using barplots, adding a horizontal line showing the overall mean of the metric (y-axis).\n",
    "\n",
    ">For each measurement type, compute the descriptive statistics for each species.\n",
    "\n",
    ">For virginica & versicolor: Compare the mean petal_width using the Mann-Whitney test (scipy.stats.mannwhitneyu) to see if there is a significant difference between the two groups. Do the same for the other measurement types.\n",
    "\n",
    ">Document takeaways & any actions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cat,num=vartypes(train_iris)\n",
    "# num_to_num(train_iris)    \n",
    "# cat_to_cat(train_iris)\n",
    "\n",
    "# cat_numcombos=cat_to_num(train_iris)\n",
    "\n",
    "\n",
    "train_telco.head()\n",
    "cat,num=vartypes(train_telco)\n",
    "display(\n",
    "pd.DataFrame(cat,columns={\"Categorical\"}).T,\n",
    "pd.DataFrame(num,columns={\"Numeric\"}).T\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_num(train_telco)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_to_cat(train_telco,rejected_chi=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_numcombos=cat_to_num(train_telco)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Multivariate Stats\n",
    "\n",
    ">Visualize the interaction of each measurement type with the others using a pairplot (or scatter matrix or something similar) and add color to represent species.\n",
    "\n",
    ">Visualize two numeric variables by means of the species. Hint: sns.relplot with hue or col\n",
    "\n",
    ">Create a swarmplot using a melted dataframe of all your numeric variables. The x-axis should be the variable name, the y-axis the measure. Add another dimension using color to represent species. Document takeaways from this visualization.\n",
    "\n",
    ">Ask a specific question of the data, such as: is the sepal area signficantly different in virginica compared to setosa? Answer the question through both a plot and using a mann-whitney or t-test. If you use a t-test, be sure assumptions are met (independence, normality, equal variance).\n",
    "\n",
    ">Document takeaways and any actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II\n",
    "\n",
    "\n",
    ">Explore your titanic dataset more completely.\n",
    "\n",
    ">Determine drivers of the target variable\n",
    "\n",
    ">Determine if certain columns should be dropped\n",
    "\n",
    ">Determine if it would be valuable to bin some numeric columns\n",
    "\n",
    ">Determine if it would be valuable to combine multiple columns into one.\n",
    "\n",
    ">Does it make sense to combine any features?\n",
    "\n",
    ">Do you find any surprises?\n",
    "\n",
    ">Document any and all findings and takeaways in your notebook using markdown.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III\n",
    "\n",
    "\n",
    ">Explore your telco data to discover drivers of churn\n",
    "\n",
    ">Determine if certain columns should be dropped\n",
    "\n",
    ">Determine if it would be valuable to bin some numeric columns\n",
    "\n",
    ">Determine if it would be valuable to combine multiple columns into one.\n",
    "\n",
    ">What are your drivers of churn?\n",
    "\n",
    ">Does it make sense to combine any features?\n",
    "\n",
    ">Do you find any surprises?\n",
    "\n",
    ">Document any and all findings and takeaways in your notebook using markdown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
