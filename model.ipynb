{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rcm_library import *\n",
    "from acquire import *\n",
    "from prepare import *\n",
    "# Sklearn stuff:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you create several variable arrays each representing hyperparameter eg $h_1$ ,...,$h_n$ . Then you iterate through each $h_i$  and apply the cartesian product you will have a list with every potential hyperparameter combination.\n",
    "\n",
    "\n",
    "Since it is computationaly expensive maybe keep each range under 3 unless something looks really interesting. \n",
    "\n",
    "As the overall length of tuples generated will be \n",
    "len( $h_1$ ) $\\times$ len( $ h_2$ ) ,..., $\\times$ len( $ h_n $ ) \n",
    "\n",
    "e.g. if len($h_1$)=len($h_2$)=len($h_3$)=5 then  $5^3$ already becomes 125 operations\n",
    "\n",
    "\n",
    "When we are considering combinations, always try to minimize them by creating entry points that restrict things\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Decision Tree\n",
    "\n",
    "\n",
    ">1. Using the titanic data, in your classification-exercises repository, create a notebook, model.ipynb where you will do the following:\n",
    ">\n",
    ">1. What is your baseline prediction? What is your baseline accuracy? remember: your baseline prediction for a classification problem is predicting the most prevelant class in the training dataset (the mode). When you make those predictions, what is your accuracy? This is your baseline accuracy.\n",
    ">\n",
    ">1. Fit the decision tree classifier to your training sample and transform (i.e. make predictions on the training sample)\n",
    ">\n",
    ">1. Evaluate your in-sample results using the model score, confusion matrix, and classification report.\n",
    ">\n",
    ">1. Compute: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support.\n",
    ">\n",
    ">1. Run through steps 2-4 using a different max_depth value.\n",
    ">\n",
    ">1. Which model performs better on your in-sample data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1. Which model performs best on your out-of-sample data, the validate set?\n",
    ">\n",
    ">1. Work through these same exercises using the Telco dataset.\n",
    "Experiment with this model on other datasets with a higher number of output classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Random Forest\n",
    "\n",
    "Continue working in your model file with titanic data to do the following:\n",
    ">\n",
    ">1. Fit the Random Forest classifier to your training sample and transform (i.e. make predictions on the training sample) setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 10.\n",
    ">\n",
    ">1. Evaluate your results using the model score, confusion matrix, and classification report.\n",
    ">\n",
    ">1. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support.\n",
    ">\n",
    ">1. Run through steps increasing your min_samples_leaf and decreasing your max_depth.\n",
    ">\n",
    ">1. What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?\n",
    ">\n",
    "After making a few models, which one has the best performance (or closest metrics) on both train and validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: KNN\n",
    "\n",
    "Continue working in your model file with the titanic dataset.\n",
    " \n",
    ">1. Fit a K-Nearest Neighbors classifier to your training sample and transform (i.e. make predictions on the training sample)\n",
    "> \n",
    ">1. Evaluate your results using the model score, confusion matrix, and classification report.\n",
    "> \n",
    ">1. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support.\n",
    ">\n",
    ">1. Run through steps 2-4 setting k to 10\n",
    "> \n",
    ">1. Run through setps 2-4 setting k to 20\n",
    "> \n",
    ">1. What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?\n",
    "> \n",
    ">1. Which model performs best on our out-of-sample data from validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/richardmacken/codeup-data-science/classification-exercises/model.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/classification-exercises/model.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m titanic\u001b[39m=\u001b[39mget_titanic_data()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/classification-exercises/model.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m titanic\u001b[39m=\u001b[39mprep_titanic(titanic)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/classification-exercises/model.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m titanic\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mpassenger_id\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdeck\u001b[39m\u001b[39m'\u001b[39m], inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/codeup-data-science/classification-exercises/acquire.py:18\u001b[0m, in \u001b[0;36mget_titanic_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# if file is available locally, read it\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(filename):\n\u001b[0;32m---> 18\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mread_csv(filename)\n\u001b[1;32m     20\u001b[0m \u001b[39m# if file not available locally, acquire data from SQL database\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# and write it as csv locally for future use\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[39m# read the SQL query into a dataframe\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     df \u001b[39m=\u001b[39m new_titanic_data()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "titanic=get_titanic_data()\n",
    "titanic=prep_titanic(titanic)\n",
    "titanic.drop(columns=[\"passenger_id\",'deck'], inplace=True)\n",
    "titanic.age=titanic.age.astype(str).replace('nan',f'{titanic.age.mean():.1f}').astype(float)\n",
    "titanic.info()\n",
    "\n",
    "# titanic.age.mode()\n",
    "# titanic.age.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# titanic.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titanic,validate_titanic,test_titanic =split_data(titanic,tostratify='survived_1')\n",
    "titanicDflist=[train_titanic,validate_titanic,test_titanic]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_knn_variables=[(i.drop(columns=['survived_1']),i.survived_1) for i in titanicDflist]\n",
    "## This is for train\n",
    "X_train=titanic_knn_variables[0][0]## the X var for train\n",
    "y_train=titanic_knn_variables[0][1]# the y var for train\n",
    "# display(X_train,y_train)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_train)\n",
    "# display(knn.classes_)##best way to see order of knn.predict\n",
    "y_pred_proba = knn.predict_proba(X_train)\n",
    "y_pred_proba.shape\n",
    "# pd.crosstab(y_train, y_pred)\n",
    "# print(classification_report(y_train, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate=titanic_knn_variables[1][0]## the X var for val\n",
    "y_validate=titanic_knn_variables[1][1]# the y var for validate\n",
    "# display(X_train,y_train)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_validate, y_validate)\n",
    "y_pred = knn.predict(X_validate)\n",
    "# display(knn.classes_)##best way to see order of knn.predict\n",
    "y_pred_proba = knn.predict_proba(X_validate)\n",
    "# y_pred_proba.shape\n",
    "# pd.crosstab(y_validate, y_pred)\n",
    "# print(classification_report(y_validate, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration:\n",
    "model_set = []\n",
    "model_accuracies = {}\n",
    "X_train=titanic_knn_variables[0][0]## the X var for train\n",
    "y_train=titanic_knn_variables[0][1]# the y var for train\n",
    "X_validate=titanic_knn_variables[1][0]## the X var for val\n",
    "y_validate=titanic_knn_variables[1][1]\n",
    "for i in range(1,10):\n",
    "    clf = KNeighborsClassifier(n_neighbors=i)\n",
    "    clf.fit(X_train, y_train)\n",
    "    model_set.append(clf)\n",
    "    model_accuracies[f'{i}_neighbors'] = {\n",
    "        'train_score': round(clf.score(X_train, y_train), 2),\n",
    "        'validate_score': round(clf.score(X_validate, y_validate), 2),\n",
    "        'diff': round(abs(clf.score(X_train, y_train)-clf.score(X_validate, y_validate)),2)}\n",
    "\n",
    "pd.DataFrame(model_accuracies)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "knn_tupples=knn_hyperpars_tupples()\n",
    "\n",
    "len(knn_tupples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "\n",
    "# iteration:\n",
    "dfdict={}\n",
    "for i,t in enumerate(knn_tupples):\n",
    "    \n",
    "    model_set = []\n",
    "    model_accuracies = {}\n",
    "    X_train=titanic_knn_variables[0][0]## the X var for train\n",
    "    y_train=titanic_knn_variables[0][1]# the y var for train\n",
    "    X_validate=titanic_knn_variables[1][0]## the X var for val\n",
    "    y_validate=titanic_knn_variables[1][1]\n",
    "    clf = KNeighborsClassifier( n_neighbors=t[0],\n",
    "    weights=t[1],\n",
    "    leaf_size=t[2],\n",
    "    p=t[3])\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    model_set.append(clf)\n",
    "    model_accuracies[f'knn_tupples[{i}]'] = {\n",
    "        'train_score': round(clf.score(X_train, y_train), 2),\n",
    "        'validate_score': round(clf.score(X_validate, y_validate), 2),\n",
    "        'diff': round(abs(clf.score(X_train, y_train)-clf.score(X_validate, y_validate)),2)}\n",
    "\n",
    "    dfdict.update((model_accuracies))   \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [display(i) for i in dflist]\n",
    "titatanic_knn=pd.DataFrame(dfdict)\n",
    "titatanic_knn=titatanic_knn.T\n",
    "titatanic_knn.nsmallest(columns='diff',keep='all',n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_titatanic_knn=titatanic_knn.nsmallest(columns='diff',keep='all',n=1)\n",
    "interesting_titatanic_knn=interesting_titatanic_knn.T\n",
    "interestingtupples=list(interesting_titatanic_knn.columns)\n",
    "interestingtupples=[i.replace('knn_tupples','').replace('[','').replace(']','') for i in interestingtupples]\n",
    "[(knn_tupples[int(i)]) for i in interestingtupples]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Logistic Regression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In these exercises, we'll continue working with the titanic dataset and building logistic regression models. Throughout this exercise, be sure you are training, evaluation, and comparing models on the train and validate datasets. The test dataset should only be used for your final model.\n",
    "\n",
    "For all of the models you create, choose a threshold that optimizes for accuracy.\n",
    "\n",
    "Do your work for these exercises in either a notebook or a python script named model within your classification-exercises repository. Add, commit, and push your work.\n",
    "\n",
    "1. Create a model that includes age in addition to fare and pclass. Does this model  perform better than your baseline?\n",
    " \n",
    "1. Include sex in your model as well. Note that you'll need to encode or create a dummy 1. variable of this feature before including it in a model.\n",
    " \n",
    "1. Try out other combinations of features and models.\n",
    " \n",
    "1. Use you best 3 models to predict and evaluate on your validate sample.\n",
    " \n",
    "1. Choose you best model from the validation performation, and evaluate it on the test dataset. How do the performance metrics compare to validate? to train?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logit_tupples=logit_hyperpars_tupples()\n",
    "display(\n",
    "logit_tupples[0],\n",
    "len(logit_tupples),\n",
    "len(logit_tupples[0])\n",
    "\n",
    "# logit_tupples[0][0][0][0][0][0],\n",
    "# logit_tupples[0][0][0][0][0][1],\n",
    "# logit_tupples[0][0][0][0][1],\n",
    "# logit_tupples[0][0][0][1],\n",
    "# logit_tupples[0][0][1],\n",
    "# logit_tupples[0][1]\n",
    "\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_titanic,validate_titanic,test_titanic =split_data(titanic,tostratify='survived_1')\n",
    "titanicDflist=[train_titanic,validate_titanic,test_titanic]\n",
    "titanic_knn_variables=[(i.drop(columns=['survived_1']),i.survived_1) for i in titanicDflist]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration:\n",
    "dfdict={}\n",
    "for i,t in enumerate(logit_tupples):\n",
    "    \n",
    "    model_set = []\n",
    "    model_accuracies = {}\n",
    "    X_train=titanic_knn_variables[0][0]## the X var for train\n",
    "    y_train=titanic_knn_variables[0][1]# the y var for train\n",
    "    X_validate=titanic_knn_variables[1][0]## the X var for val\n",
    "    y_validate=titanic_knn_variables[1][1]\n",
    "    \n",
    "    clf = LogisticRegression( \n",
    "        penalty=t[0],\n",
    "        C=t[1],\n",
    "        class_weight=t[2],\n",
    "        solver=t[3],\n",
    "        max_iter=t[4],\n",
    "        intercept_scaling=t[5]\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    model_set.append(clf)\n",
    "    model_accuracies[f'logit_tupples[{i}]'] = {\n",
    "        'train_score': round(clf.score(X_train, y_train), 2),\n",
    "        'validate_score': round(clf.score(X_validate, y_validate), 2),\n",
    "        'diff': round(abs(clf.score(X_train, y_train)-clf.score(X_validate, y_validate)),2)}\n",
    "\n",
    "    dfdict.update((model_accuracies))   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titatanic_logit=pd.DataFrame(dfdict)\n",
    "titatanic_logit=titatanic_logit.T\n",
    "titatanic_logit.nsmallest(columns='diff',keep='all',n=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_titatanic_logit=titatanic_logit.nsmallest(columns='diff',keep='all',n=1)\n",
    "\n",
    "interesting_titatanic_logit=interesting_titatanic_logit.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "interestingtupples=list(interesting_titatanic_logit.columns)\n",
    "interestingtupples=[i.replace('logit_tupples','').replace('[','').replace(']','') for i in interestingtupples]\n",
    "\n",
    "logit_tupples_df=[logit_tupples[int(i)] for i in (interestingtupples)]\n",
    "\n",
    "logit_tupples_df=pd.DataFrame(logit_tupples_df)\n",
    "logit_tupples_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus1 \n",
    "How do different strategies for handling the missing values in the age column affect model performance?\n",
    "\n",
    "### Bonus2:\n",
    "How do different strategies for encoding sex affect model performance?\n",
    "\n",
    "### Bonus3:\n",
    " scikit-learn's LogisticRegression classifier is actually applying a regularization penalty to the coefficients by default. This penalty causes the magnitude of the coefficients in the resulting model to be smaller than they otherwise would be. This value can be modified with the C hyper parameter. Small values of C correspond to a larger penalty, and large values of C correspond to a smaller penalty.\n",
    "Try out the following values for C and note how the coefficients and the model's performance on both the dataset it was trained on and on the validate split are affected.\n",
    "$$ C=.01,.1,1,10,100,1000 $$\n",
    "### Bonus Bonus:\n",
    " how does scaling the data interact with your choice of C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
